{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Data Preprocessing for Reddit Autos Selfposts<div class=\"tocSkip\">\n",
    "    \n",
    "&copy; Jens Albrecht, 2023\n",
    "    \n",
    "This notebook can be freely copied and modified.  \n",
    "Attribution, however, is highly appreciated.\n",
    "\n",
    "<hr/>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "See also: \n",
    "\n",
    "Albrecht, Ramachandran, Winkler: **Blueprints for Text Analytics in Python** (O'Reilly 2020)  \n",
    "Chapter 4: [Preparing Data for Statistics and Machine Learning](https://learning.oreilly.com/library/view/blueprints-for-text/9781492074076/ch04.html#ch-preparation) + [Link to Github](https://github.com/blueprints-for-text-analytics-python/blueprints-text/blob/master/README.md)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup<div class='tocSkip'/>\n",
    "\n",
    "Set directory locations. If working on Google Colab: copy files and install required libraries."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:19:34.670888Z",
     "start_time": "2023-06-18T09:19:34.652092Z"
    }
   },
   "outputs": [],
   "source": [
    "import sys, os\n",
    "ON_COLAB = 'google.colab' in sys.modules\n",
    "\n",
    "if ON_COLAB:\n",
    "    GIT_ROOT = 'https://github.com/jsalbr/tdwi-2021-text-mining/raw/main'\n",
    "    os.system(f'wget {GIT_ROOT}/notebooks/setup.py')\n",
    "\n",
    "%run -i setup.py"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Python Settings<div class=\"tocSkip\"/>\n",
    "\n",
    "Common imports, defaults for formatting in Matplotlib, Pandas etc."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:19:46.822046Z",
     "start_time": "2023-06-18T09:19:35.508881Z"
    }
   },
   "outputs": [],
   "source": [
    "%run \"$BASE_DIR/notebooks/settings.py\"\n",
    "\n",
    "%reload_ext autoreload\n",
    "%autoreload 2\n",
    "%config InlineBackend.figure_format = 'png'\n",
    "\n",
    "# to print output of all statements and not just the last\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# otherwise text between $ signs will be interpreted as formula and printed in italic\n",
    "pd.set_option('display.html.use_mathjax', False)\n",
    "pd.options.plotting.backend = \"plotly\"\n",
    "\n",
    "# path to import blueprints packages\n",
    "sys.path.append(f'{BASE_DIR}/packages')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading the Data Set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:19:47.642166Z",
     "start_time": "2023-06-18T09:19:46.829043Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE_DIR}/data/reddit-autos-selfposts-cleaned.csv\", sep=\";\", decimal=\".\", parse_dates=['created'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:09.188173Z",
     "start_time": "2023-06-18T09:20:07.187714Z"
    },
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "df['subreddit'].value_counts().head(20).plot(kind='barh', height=500).update_yaxes(autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:12.573749Z",
     "start_time": "2023-06-18T09:20:12.277753Z"
    }
   },
   "outputs": [],
   "source": [
    "df.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:13.369009Z",
     "start_time": "2023-06-18T09:20:13.060001Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:15.789272Z",
     "start_time": "2023-06-18T09:20:15.445103Z"
    }
   },
   "outputs": [],
   "source": [
    "df.describe(include='O').T"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Linguistic Processing with spaCy\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Instantiating a Pipeline\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:19.361602Z",
     "start_time": "2023-06-18T09:20:18.397102Z"
    }
   },
   "outputs": [],
   "source": [
    "import spacy\n",
    "nlp = spacy.load('en_core_web_sm')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:21.612891Z",
     "start_time": "2023-06-18T09:20:21.300886Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "nlp.pipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Processing Text\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:24.703376Z",
     "start_time": "2023-06-18T09:20:24.376341Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "doc = nlp(text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:28.803813Z",
     "start_time": "2023-06-18T09:20:28.506778Z"
    }
   },
   "outputs": [],
   "source": [
    "for token in doc:\n",
    "    print(token, end=\"|\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:32.002336Z",
     "start_time": "2023-06-18T09:20:31.705598Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.preparation import display_nlp\n",
    "\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Lemmas based on Part-of-Speech\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:42.546800Z",
     "start_time": "2023-06-18T09:20:42.258763Z"
    }
   },
   "outputs": [],
   "source": [
    "nouns = [t for t in doc if t.pos_ in ['NOUN', 'PROPN']]\n",
    "print(nouns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:43.352311Z",
     "start_time": "2023-06-18T09:20:43.062319Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_lemmas(doc, include_pos=None, exclude_pos=[]):\n",
    "    return [t.lemma_ \n",
    "            for t in doc \n",
    "            if (include_pos==None or t.pos_ in include_pos) and t.pos_ not in exclude_pos]\n",
    "\n",
    "lemmas = extract_lemmas(doc, include_pos=['NOUN', 'PROPN'])\n",
    "print(lemmas)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extracting Named Entities\n",
    "\n",
    "### Model-based Named Entity Recognition (NER)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:44.880508Z",
     "start_time": "2023-06-18T09:20:44.573353Z"
    }
   },
   "outputs": [],
   "source": [
    "doc = nlp(text)\n",
    "\n",
    "for ent in doc.ents:\n",
    "    print(f\"({ent.text}, {ent.label_})\", end=\" \")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:45.737879Z",
     "start_time": "2023-06-18T09:20:45.445514Z"
    }
   },
   "outputs": [],
   "source": [
    "from spacy import displacy\n",
    "\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:48.431179Z",
     "start_time": "2023-06-18T09:20:48.137151Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_entities(doc, include_types=None):\n",
    "\n",
    "    return [t.text for t in doc if t.ent_type_ in include_types]\n",
    "\n",
    "print(extract_entities(doc, ['ORG', 'PERSON']))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Rule-based NER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:50.619958Z",
     "start_time": "2023-06-18T09:20:49.688414Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load(\"en_core_web_sm\")\n",
    "nlp.add_pipe(\"merge_entities\")\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:52.217572Z",
     "start_time": "2023-06-18T09:20:51.915408Z"
    }
   },
   "outputs": [],
   "source": [
    "import cars\n",
    "\n",
    "cars.brands[:5]\n",
    "cars.models[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Choose a relaxed pattern - favors high recall but will result in false positives"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:56.072559Z",
     "start_time": "2023-06-18T09:20:55.777199Z"
    }
   },
   "outputs": [],
   "source": [
    "patterns = [{\"label\": \"BRAND\", \n",
    "             \"pattern\": [{\"LOWER\": {\"IN\": cars.brands}, \n",
    "                          \"POS\": {\"IN\": [\"PROPN\", \"NOUN\", \"ADJ\"] }\n",
    "                         }]},\n",
    "            {\"label\": \"MODEL\", \n",
    "             \"pattern\": [{\"LOWER\": {\"IN\": cars.models}, \n",
    "                          \"POS\": {\"IN\": [\"PROPN\", \"NOUN\", \"ADJ\"] }\n",
    "                         }]}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:57.110373Z",
     "start_time": "2023-06-18T09:20:56.817151Z"
    }
   },
   "outputs": [],
   "source": [
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:20:57.912585Z",
     "start_time": "2023-06-18T09:20:57.617330Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:01.376319Z",
     "start_time": "2023-06-18T09:21:01.085013Z"
    }
   },
   "outputs": [],
   "source": [
    "'spark' in cars.models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:02.534327Z",
     "start_time": "2023-06-18T09:21:02.244347Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The spark plugs in my Z4 are making trouble.\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:05.192230Z",
     "start_time": "2023-06-18T09:21:04.885231Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"I currently have a 2018 honda civic that I'd be getting rid of\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:05.523600Z",
     "start_time": "2023-06-18T09:21:05.197235Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"I hate Mercedes A-Class\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "# display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:05.869967Z",
     "start_time": "2023-06-18T09:21:05.528614Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "text = \"I hate A Mercedes\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:06.202893Z",
     "start_time": "2023-06-18T09:21:05.871747Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"Is a 2017 Tesla Model S worth it in 2023?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "displacy.render(doc, style='ent', jupyter=True)\n",
    "display_nlp(doc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: One Function to Get It All\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:09.067565Z",
     "start_time": "2023-06-18T09:21:08.776384Z"
    }
   },
   "outputs": [],
   "source": [
    "def extract_nlp(doc):\n",
    "    return {\n",
    "    'lemmas' : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN', 'VERB', 'ADJ', 'NUM']),\n",
    "    'nouns'  : extract_lemmas(doc, include_pos = ['NOUN', 'PROPN']),\n",
    "    'brands' : extract_entities(doc, ['BRAND']),\n",
    "    'models' : extract_entities(doc, ['MODEL'])\n",
    "    }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:13.798222Z",
     "start_time": "2023-06-18T09:21:12.959956Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "\n",
    "_ = nlp.add_pipe(\"merge_entities\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)\n",
    "\n",
    "[pipe[0] for pipe in nlp.pipeline]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:14.461257Z",
     "start_time": "2023-06-18T09:21:14.096224Z"
    }
   },
   "outputs": [],
   "source": [
    "text = \"The BMW X5 and the Mercedes GLK are interesting cars. Does VW have models like these?\"\n",
    "\n",
    "doc = nlp(text)\n",
    "for col, values in extract_nlp(doc).items():\n",
    "    print(f\"{col}: {values}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:17.376063Z",
     "start_time": "2023-06-18T09:21:17.071951Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp_columns = list(extract_nlp(nlp.make_doc('')).keys())\n",
    "print(nlp_columns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Blueprint: Using spaCy on a Large Data Set\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:22.546801Z",
     "start_time": "2023-06-18T09:21:21.970373Z"
    }
   },
   "outputs": [],
   "source": [
    "df = pd.read_csv(f\"{BASE_DIR}/data/reddit-autos-selfposts-cleaned.csv\", sep=\";\", decimal=\".\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:33.164737Z",
     "start_time": "2023-06-18T09:21:32.862716Z"
    }
   },
   "outputs": [],
   "source": [
    "pd.options.display.max_colwidth = 1000\n",
    "df[['subreddit', 'text']].sample(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:34.917790Z",
     "start_time": "2023-06-18T09:21:34.616703Z"
    }
   },
   "outputs": [],
   "source": [
    "for col in nlp_columns:\n",
    "    df[col] = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:36.100479Z",
     "start_time": "2023-06-18T09:21:35.809444Z"
    }
   },
   "outputs": [],
   "source": [
    "if spacy.prefer_gpu():\n",
    "    print(\"Working on GPU.\")\n",
    "else:\n",
    "    print(\"No GPU found, working on CPU.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:37.510617Z",
     "start_time": "2023-06-18T09:21:36.691678Z"
    }
   },
   "outputs": [],
   "source": [
    "nlp = spacy.load('en_core_web_sm', disable=[])\n",
    "\n",
    "_ = nlp.add_pipe(\"merge_entities\")\n",
    "ruler = nlp.add_pipe(\"entity_ruler\", config={\"overwrite_ents\": True})\n",
    "ruler.add_patterns(patterns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:46.452829Z",
     "start_time": "2023-06-18T09:21:46.158607Z"
    }
   },
   "outputs": [],
   "source": [
    "# full data set takes about 5 minutes\n",
    "# for faster processing use a sample like this\n",
    "df = df.sample(500)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:55.665900Z",
     "start_time": "2023-06-18T09:21:48.161810Z"
    }
   },
   "outputs": [],
   "source": [
    "batch_size = 50\n",
    "batches = math.ceil(len(df) / batch_size) ###\n",
    "\n",
    "for i in tqdm(range(0, len(df), batch_size), total=batches):\n",
    "    docs = nlp.pipe(df['text'][i:i+batch_size])\n",
    "    \n",
    "    for j, doc in enumerate(docs):\n",
    "        for col, values in extract_nlp(doc).items():\n",
    "            df[col].iloc[i+j] = values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:21:57.385358Z",
     "start_time": "2023-06-18T09:21:57.081314Z"
    },
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "df[['text', 'lemmas', 'brands', 'models']].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Finalizing and Saving the Result"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Quick Frequency Analyses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:02.696302Z",
     "start_time": "2023-06-18T09:22:01.988074Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.exploration import count_words\n",
    "\n",
    "freq_df = count_words(df, 'nouns')\n",
    "\n",
    "freq_df.head(20).plot(kind='barh', height=500).update_yaxes(autorange=\"reversed\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:05.836197Z",
     "start_time": "2023-06-18T09:22:04.959044Z"
    }
   },
   "outputs": [],
   "source": [
    "from blueprints.exploration import wordcloud\n",
    "\n",
    "wordcloud(freq_df['freq'], max_words=100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:09.030189Z",
     "start_time": "2023-06-18T09:22:08.201176Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'models')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:12.540785Z",
     "start_time": "2023-06-18T09:22:11.899920Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'brands')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Synonym Resolution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:19.839296Z",
     "start_time": "2023-06-18T09:22:19.516287Z"
    }
   },
   "outputs": [],
   "source": [
    "synonyms = { brand: brand for brand in cars.brands }\n",
    "synonyms['mercedes-menz'] = 'mercedes'\n",
    "synonyms['volkswagen'] = 'vw'\n",
    "\n",
    "df['brands'] = df['brands'].progress_map(lambda brands: [synonyms[b.lower()] for b in brands])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:21.176805Z",
     "start_time": "2023-06-18T09:22:20.560872Z"
    }
   },
   "outputs": [],
   "source": [
    "freq_df = count_words(df, 'brands')\n",
    "wordcloud(freq_df['freq'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Saving the Prepared Data\n",
    "\n",
    "Alternatively into a SQL database or JSON file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2023-06-18T09:22:29.543944Z",
     "start_time": "2023-06-18T09:22:29.231724Z"
    }
   },
   "outputs": [],
   "source": [
    "# convert lists of tokens into space-separated strings for csv-saving\n",
    "df[nlp_columns] = df[nlp_columns].applymap(lambda items: ' '.join(items).lower())\n",
    "\n",
    "# df.to_csv(f\"reddit-autos-selfposts-prepared.csv\", sep=\";\", decimal=\".\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "ExecuteTime": {
     "end_time": "2021-06-19T12:05:41.946194Z",
     "start_time": "2021-06-19T12:05:41.893743Z"
    }
   },
   "outputs": [],
   "source": [
    "# restore lists\n",
    "# df[nlp_columns] = df[nlp_columns].applymap(lambda items: items.split())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "265.627px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  },
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
